# HLO.01 - Alignment Bypass/Jailbreak
## Description
Subvert a model's safety mechanisms, alignment tuning, or system-level guardrails to enable behaviors the model was explicitly trained or configured to avoid, such as generating harmful, biased, or policy-violating content.
### Back to Objectives
[[APE/Objectives/Objectives.md|Objectives]]

#objectives
